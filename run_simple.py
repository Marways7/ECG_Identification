#!/usr/bin/env python3
"""
ECGèº«ä»½è¯†åˆ«ç³»ç»Ÿ - ç®€åŒ–è¿è¡Œç‰ˆæœ¬
==============================

ä½¿ç”¨ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•ï¼ˆä¸ä¾èµ–PyTorchï¼‰
"""

import os
import sys
import numpy as np
from datetime import datetime
from loguru import logger

# é…ç½®æ—¥å¿—
logger.remove()
logger.add(
    sys.stdout,
    format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{message}</cyan>",
    level="INFO"
)

os.makedirs("logs", exist_ok=True)
logger.add(
    "logs/ecg_id_{time:YYYY-MM-DD}.log",
    rotation="1 day",
    level="DEBUG"
)

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.utils.data_loader import ECGDataLoader
from src.preprocessing.signal_pipeline import SignalPreprocessor, PreprocessingConfig

# å¯¼å…¥æœºå™¨å­¦ä¹ åº“ï¼ˆä¸ä¾èµ–PyTorchï¼‰
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import xgboost as xgb
import lightgbm as lgb


def load_and_preprocess():
    """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
    logger.info("=" * 60)
    logger.info("é˜¶æ®µ1: æ•°æ®åŠ è½½ä¸é¢„å¤„ç†")
    logger.info("=" * 60)
    
    loader = ECGDataLoader('ECG_Data')
    signals, labels, subject_ids = loader.prepare_training_data(sampling_rate=250.0)
    
    logger.info(f"åŠ è½½äº† {len(signals)} ä¸ªè¢«è¯•çš„æ•°æ®")
    
    # é¢„å¤„ç†é…ç½® (250Hzé‡‡æ ·ç‡)
    config = PreprocessingConfig(
        sampling_rate=250.0,
        wavelet='db4',
        wavelet_mode='soft',
        baseline_method='morphological',
        rpeak_method='pan_tompkins',
        beat_pre_r=0.25,
        beat_post_r=0.45,
        beat_target_length=175  # 0.7s * 250Hz
    )
    
    preprocessor = SignalPreprocessor(config)
    
    all_beats = []
    all_labels = []
    
    for signal, label, subject_id in zip(signals, labels, subject_ids):
        logger.info(f"å¤„ç† {subject_id} ({label})...")
        
        try:
            result = preprocessor.process(signal)
            
            if result.is_valid() and len(result.beats) > 30:
                all_beats.append(result.beats)
                all_labels.extend([label] * len(result.beats))
                
                logger.info(f"  {len(result.beats)} å¿ƒæ‹, HR={result.heart_rate:.1f} BPM")
            else:
                logger.warning(f"  {subject_id} æ•°æ®ä¸è¶³ï¼Œè·³è¿‡")
                
        except Exception as e:
            logger.error(f"  å¤„ç† {subject_id} å¤±è´¥: {e}")
    
    if not all_beats:
        raise ValueError("æœªæ‰¾åˆ°å¯ç”¨å¿ƒæ‹æ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®ç›®å½•æˆ–é¢„å¤„ç†é…ç½®ã€‚")
    all_beats = np.vstack(all_beats)
    all_labels = np.array(all_labels)
    
    logger.info(f"é¢„å¤„ç†å®Œæˆ: {len(all_beats)} å¿ƒæ‹, {len(np.unique(all_labels))} ç±»åˆ«")
    
    return all_beats, all_labels


def extract_simple_features(beats):
    """æå–ç®€å•çš„å¿ƒæ‹ç‰¹å¾"""
    logger.info("æå–å¿ƒæ‹ç‰¹å¾...")
    
    features = []
    for beat in beats:
        n = len(beat)
        r_pos = n // 2
        
        # åŸºæœ¬ç»Ÿè®¡ç‰¹å¾
        feat = [
            np.mean(beat),
            np.std(beat),
            np.max(beat),
            np.min(beat),
            np.max(beat) - np.min(beat),
            beat[r_pos],  # Rå³°å¹…åº¦
            np.sum(beat ** 2),  # èƒ½é‡
            np.sum(np.abs(np.diff(beat))),  # æ›²çº¿é•¿åº¦
        ]
        
        # QRSåŒºåŸŸç‰¹å¾
        qrs_half = int(0.04 * 250)  # 40ms
        qrs = beat[r_pos-qrs_half:r_pos+qrs_half]
        feat.extend([
            np.max(qrs) - np.min(qrs),
            np.sum(qrs ** 2),
        ])
        
        # æ³¢å½¢å½¢çŠ¶ç‰¹å¾
        feat.extend([
            np.percentile(beat, 25),
            np.percentile(beat, 50),
            np.percentile(beat, 75),
            np.sum(np.diff(np.sign(beat - np.mean(beat))) != 0),  # è¿‡é›¶ç‡
        ])
        
        # å°æ³¢èƒ½é‡ç‰¹å¾
        import pywt
        coeffs = pywt.wavedec(beat, 'db4', level=4)
        for c in coeffs:
            feat.append(np.sum(c ** 2))
        
        features.append(feat)
    
    return np.array(features)


def train_ensemble(X, y):
    """è®­ç»ƒé›†æˆåˆ†ç±»å™¨"""
    logger.info("=" * 60)
    logger.info("é˜¶æ®µ2: æ¨¡å‹è®­ç»ƒ")
    logger.info("=" * 60)
    
    # ç¼–ç æ ‡ç­¾
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # å¤„ç†NaN
    X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=0.0, neginf=0.0)
    
    # åˆ’åˆ†æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42
    )
    
    logger.info(f"è®­ç»ƒé›†: {len(X_train)}, æµ‹è¯•é›†: {len(X_test)}")
    
    # å®šä¹‰åˆ†ç±»å™¨
    classifiers = {
        'RandomForest': RandomForestClassifier(
            n_estimators=200, max_depth=15, min_samples_split=5,
            class_weight='balanced', random_state=42, n_jobs=-1
        ),
        'XGBoost': xgb.XGBClassifier(
            n_estimators=200, max_depth=8, learning_rate=0.1,
            subsample=0.8, colsample_bytree=0.8, random_state=42,
            use_label_encoder=False, eval_metric='mlogloss'
        ),
        'LightGBM': lgb.LGBMClassifier(
            n_estimators=200, max_depth=8, learning_rate=0.1,
            subsample=0.8, colsample_bytree=0.8, random_state=42, verbose=-1
        ),
        'SVM': SVC(C=10, kernel='rbf', probability=True, class_weight='balanced', random_state=42)
    }
    
    # äº¤å‰éªŒè¯è¯„ä¼°
    logger.info("\näº¤å‰éªŒè¯ç»“æœ:")
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    cv_results = {}
    for name, clf in classifiers.items():
        scores = cross_val_score(clf, X_train, y_train, cv=cv, scoring='accuracy')
        cv_results[name] = {'mean': scores.mean(), 'std': scores.std()}
        logger.info(f"  {name}: {scores.mean():.4f} Â± {scores.std():.4f}")
    
    # è®­ç»ƒæŠ•ç¥¨åˆ†ç±»å™¨
    logger.info("\nè®­ç»ƒé›†æˆåˆ†ç±»å™¨...")
    
    voting_clf = VotingClassifier(
        estimators=[
            ('rf', classifiers['RandomForest']),
            ('xgb', classifiers['XGBoost']),
            ('lgb', classifiers['LightGBM']),
            ('svm', classifiers['SVM'])
        ],
        voting='soft',
        n_jobs=-1
    )
    
    voting_clf.fit(X_train, y_train)
    
    # æµ‹è¯•é›†è¯„ä¼°
    y_pred = voting_clf.predict(X_test)
    y_pred_proba = voting_clf.predict_proba(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    logger.info(f"\næµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test, y_pred)
    labels = le.classes_
    
    logger.info("\næ··æ·†çŸ©é˜µ:")
    header = "     " + "  ".join([f"{l:>5}" for l in labels])
    logger.info(header)
    for i, row in enumerate(cm):
        row_str = f"{labels[i]:>4} " + "  ".join([f"{v:>5}" for v in row])
        logger.info(row_str)
    
    # åˆ†ç±»æŠ¥å‘Š
    logger.info("\nåˆ†ç±»æŠ¥å‘Š:")
    report = classification_report(y_test, y_pred, target_names=labels)
    report_dict = classification_report(y_test, y_pred, target_names=labels, output_dict=True)
    for line in report.split('\n'):
        if line.strip():
            logger.info(line)
    
    # æ”¶é›†æ‰€æœ‰ç»“æœ
    results = {
        'accuracy': accuracy,
        'confusion_matrix': cm,
        'labels': labels,
        'cv_results': cv_results,
        'classification_report': report_dict,
        'y_test': y_test,
        'y_pred': y_pred,
        'y_pred_proba': y_pred_proba
    }
    
    return voting_clf, scaler, le, results


def save_results(beats, labels, features, model, scaler, le, results, preprocess_info):
    """ä¿å­˜æ‰€æœ‰ç»“æœ"""
    import joblib
    import json
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "output"
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    logger.info("\nä¿å­˜ç»“æœ...")
    
    # 1. ä¿å­˜æ¨¡å‹
    model_path = os.path.join(output_dir, f"model_{timestamp}.joblib")
    joblib.dump({
        'model': model,
        'scaler': scaler,
        'label_encoder': le
    }, model_path)
    logger.info(f"  æ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    # 2. ä¿å­˜å¿ƒæ‹æ•°æ®
    beats_path = os.path.join(output_dir, f"beats_{timestamp}.npz")
    np.savez_compressed(beats_path, beats=beats, labels=labels, features=features)
    logger.info(f"  å¿ƒæ‹æ•°æ®å·²ä¿å­˜: {beats_path}")
    
    # 3. ä¿å­˜è¯„ä¼°ç»“æœ
    eval_results = {
        'timestamp': timestamp,
        'accuracy': float(results['accuracy']),
        'labels': results['labels'].tolist(),
        'confusion_matrix': results['confusion_matrix'].tolist(),
        'cv_results': {k: {'mean': float(v['mean']), 'std': float(v['std'])} 
                       for k, v in results['cv_results'].items()},
        'classification_report': results['classification_report'],
        'preprocess_info': preprocess_info,
        'total_beats': len(beats),
        'num_subjects': len(np.unique(labels))
    }
    
    results_path = os.path.join(output_dir, f"results_{timestamp}.json")
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(eval_results, f, indent=2, ensure_ascii=False)
    logger.info(f"  è¯„ä¼°ç»“æœå·²ä¿å­˜: {results_path}")
    
    # 4. ä¿å­˜æœ€æ–°ç»“æœçš„è½¯é“¾æ¥/å¤åˆ¶
    latest_model = os.path.join(output_dir, "model_latest.joblib")
    latest_results = os.path.join(output_dir, "results_latest.json")
    latest_beats = os.path.join(output_dir, "beats_latest.npz")
    
    # å¤åˆ¶ä¸ºlatestç‰ˆæœ¬
    import shutil
    shutil.copy(model_path, latest_model)
    shutil.copy(results_path, latest_results)
    shutil.copy(beats_path, latest_beats)
    
    logger.info(f"\nğŸ“ æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ° {output_dir}/ ç›®å½•")
    
    return output_dir


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 60)
    logger.info("ECGèº«ä»½è¯†åˆ«ç³»ç»Ÿ - ç®€åŒ–ç‰ˆæœ¬")
    logger.info(f"å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("é‡‡æ ·ç‡: 250Hz")
    logger.info("=" * 60)
    
    # 1. åŠ è½½å’Œé¢„å¤„ç†
    beats, labels = load_and_preprocess()
    
    # æ”¶é›†é¢„å¤„ç†ä¿¡æ¯
    unique_labels, counts = np.unique(labels, return_counts=True)
    preprocess_info = {
        'subjects': {str(label): int(count) for label, count in zip(unique_labels, counts)},
        'sampling_rate': 250.0,
        'beat_length': beats.shape[1] if len(beats) > 0 else 0
    }
    
    # 2. ç‰¹å¾æå–
    features = extract_simple_features(beats)
    logger.info(f"ç‰¹å¾ç»´åº¦: {features.shape}")
    
    # 3. è®­ç»ƒæ¨¡å‹
    model, scaler, le, results = train_ensemble(features, labels)
    
    # 4. ä¿å­˜ç»“æœ
    output_dir = save_results(beats, labels, features, model, scaler, le, results, preprocess_info)
    
    # 5. æ‰“å°æœ€ç»ˆç»“æœ
    accuracy = results['accuracy']
    logger.info("\n" + "=" * 60)
    logger.info("æœ€ç»ˆç»“æœ")
    logger.info("=" * 60)
    logger.info(f"âœ… å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
    logger.info(f"âœ… è¢«è¯•æ•°é‡: {len(np.unique(labels))}")
    logger.info(f"âœ… æ€»å¿ƒæ‹æ•°: {len(beats)}")
    logger.info(f"âœ… ç»“æœç›®å½•: {output_dir}/")
    
    return accuracy


if __name__ == '__main__':
    main()
